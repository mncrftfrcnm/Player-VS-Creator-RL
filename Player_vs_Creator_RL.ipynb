{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb363fa3",
      "metadata": {
        "id": "eb363fa3"
      },
      "source": [
        "\n",
        "# Player vs Creator â€” Adversarial RL (PPO, PyTorch)\n",
        "\n",
        "We study a two-agent adversarial problem in which a Player must reach a goal while a Creator generates obstacles that impede progress. Both agents are trained with proximal policy optimization (PPO) in a continuous 2D environment. The world includes static circles, axis-aligned rectangles, and moving circles; collisions are elastic so the Player never stalls. A curriculum on obstacle count, observation/return normalization, learning-rate annealing, and KL-based early stopping stabilize training. We report success rate and final-distance metrics and visualize optimization dynamics with comprehensive plots. An interactive demo renders episodes produced by the Creator and navigated by the Player."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f7e6c2d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7e6c2d1",
        "outputId": "193996ce-6fe6-452a-ec62-6b8b20696697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q gradio imageio\n",
        "import os, math, time, random, json\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import imageio.v2 as imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43b056d8",
      "metadata": {
        "id": "43b056d8"
      },
      "source": [
        "##environment\n",
        "**State space.** 2D continuous world \\([{-}1,1]^2\\). Observations for the Player: \\([x,y,\\dot x,\\dot y],\\) goal location, vector-to-goal, and \\(K\\) obstacle descriptors; for the Creator: start and goal.  \n",
        "\n",
        "**Action space.** Player: continuous acceleration \\(\\mathbf{a}\\in[-1,1]^2\\) (tanh-squashed). Creator: per-obstacle vector \\([\\text{type},x,y,s_1,s_2,v_x,v_y]\\) for \\(K\\) slots; typeâˆˆ{circle, rect, moving-circle}.  \n",
        "\n",
        "**Physics.** Semi-implicit integration with damping and speed cap; walls and obstacles reflect velocity (elastic), ensuring non-stopping contact.  \n",
        "\n",
        "**Rewards.** Player: shaped by progress toward goal plus terminal success bonus and small step cost. Creator: reward if the Player fails plus a distance-proportional term; penalties for illegal placements (out of bounds or near start/goal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7075ad59",
      "metadata": {
        "id": "7075ad59"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title Environment (circles, rectangles, moving circles)\n",
        "@dataclass\n",
        "class LevelConfig:\n",
        "    world_min: float = -1.0\n",
        "    world_max: float =  1.0\n",
        "    max_obstacles: int = 6\n",
        "    goal_r: float = 0.08\n",
        "    start_r: float = 0.08\n",
        "    min_clearance: float = 0.05\n",
        "    max_steps: int = 220\n",
        "    dt: float = 0.08\n",
        "    vmax: float = 0.9\n",
        "    accel_max: float = 0.8\n",
        "    damping: float = 0.03\n",
        "    collision_eps: float = 1e-4\n",
        "    circle_r_min: float = 0.06\n",
        "    circle_r_max: float = 0.25\n",
        "    rect_w_min: float = 0.10\n",
        "    rect_w_max: float = 0.45\n",
        "    rect_h_min: float = 0.10\n",
        "    rect_h_max: float = 0.45\n",
        "    obs_vmax: float = 0.7\n",
        "\n",
        "class PlayerCreatorEnvVec:\n",
        "    TYPE_CIRCLE = 0\n",
        "    TYPE_RECT   = 1\n",
        "    TYPE_MCIRC  = 2\n",
        "\n",
        "    def __init__(self, num_envs=32, level_cfg: LevelConfig = LevelConfig(), active_obstacles=None):\n",
        "        self.N = num_envs\n",
        "        self.cfg = level_cfg\n",
        "        self.low, self.high = self.cfg.world_min, self.cfg.world_max\n",
        "        self.max_obs = self.cfg.max_obstacles\n",
        "        self.active_k = int(active_obstacles) if active_obstacles is not None else min(3, self.max_obs)\n",
        "\n",
        "        self.dim_obs_player = 8 + 7*self.max_obs\n",
        "        self.dim_obs_creator = 4\n",
        "\n",
        "        self.reset_levels()\n",
        "        self.reset_players()\n",
        "\n",
        "    def sample_start_goal(self, rng):\n",
        "        while True:\n",
        "            s = rng.uniform(self.low+0.2, self.high-0.2, size=(2,))\n",
        "            g = rng.uniform(self.low+0.2, self.high-0.2, size=(2,))\n",
        "            if np.linalg.norm(s - g) > 0.8:\n",
        "                return s, g\n",
        "\n",
        "    def reset_levels(self, seeds=None):\n",
        "        rng = np.random.default_rng()\n",
        "        self.starts = np.zeros((self.N, 2), dtype=np.float32)\n",
        "        self.goals  = np.zeros((self.N, 2), dtype=np.float32)\n",
        "        for i in range(self.N):\n",
        "            s, g = self.sample_start_goal(rng)\n",
        "            self.starts[i] = s\n",
        "            self.goals[i]  = g\n",
        "        self.obs_params = np.zeros((self.N, self.max_obs, 7), dtype=np.float32)\n",
        "        self.obs_type   = np.zeros((self.N, self.max_obs), dtype=np.int32)\n",
        "        self.n_obs      = np.full((self.N,), self.active_k, dtype=np.int32)\n",
        "\n",
        "    def reset_players(self):\n",
        "        self.pos  = self.starts.copy()\n",
        "        self.vel  = np.zeros_like(self.pos, dtype=np.float32)\n",
        "        self.t    = np.zeros(self.N, dtype=np.int32)\n",
        "        self.done = np.zeros(self.N, dtype=np.bool_)\n",
        "\n",
        "    def _decode_set_obstacle(self, i, j, vec):\n",
        "        v = np.clip(vec, -1.0, 1.0)\n",
        "        tc = v[0]\n",
        "        if tc < -1/3:\n",
        "            typ = self.TYPE_CIRCLE\n",
        "        elif tc < 1/3:\n",
        "            typ = self.TYPE_RECT\n",
        "        else:\n",
        "            typ = self.TYPE_MCIRC\n",
        "        x = self.low + (v[1] + 1.0) * 0.5 * (self.high - self.low)\n",
        "        y = self.low + (v[2] + 1.0) * 0.5 * (self.high - self.low)\n",
        "        if typ == self.TYPE_CIRCLE:\n",
        "            r = self.cfg.circle_r_min + (v[3] + 1.0) * 0.5 * (self.cfg.circle_r_max - self.cfg.circle_r_min)\n",
        "            s1, s2, vx, vy = r, 0.0, 0.0, 0.0\n",
        "        elif typ == self.TYPE_RECT:\n",
        "            w = self.cfg.rect_w_min + (v[3] + 1.0) * 0.5 * (self.cfg.rect_w_max - self.cfg.rect_w_min)\n",
        "            h = self.cfg.rect_h_min + (v[4] + 1.0) * 0.5 * (self.cfg.rect_h_max - self.cfg.rect_h_min)\n",
        "            s1, s2, vx, vy = w, h, 0.0, 0.0\n",
        "        else:\n",
        "            r = self.cfg.circle_r_min + (v[3] + 1.0) * 0.5 * (self.cfg.circle_r_max - self.cfg.circle_r_min)\n",
        "            vx = v[5] * self.cfg.obs_vmax\n",
        "            vy = v[6] * self.cfg.obs_vmax\n",
        "            s1, s2 = r, 0.0\n",
        "        self.obs_type[i, j]      = typ\n",
        "        self.obs_params[i, j, :] = np.array([tc, x, y, s1, s2, vx, vy], dtype=np.float32)\n",
        "\n",
        "    def set_obstacles_from_creator_action(self, a_creator):\n",
        "        if isinstance(a_creator, torch.Tensor):\n",
        "            a_creator = a_creator.detach().cpu().numpy()\n",
        "        a_creator = np.clip(a_creator, -1.0, 1.0).reshape(self.N, self.max_obs, 7)\n",
        "        for i in range(self.N):\n",
        "            for j in range(self.max_obs):\n",
        "                self._decode_set_obstacle(i, j, a_creator[i, j])\n",
        "\n",
        "    def _reflect_velocity(self, v, n):\n",
        "        dot = v[0]*n[0] + v[1]*n[1]\n",
        "        return np.array([v[0] - 2*dot*n[0], v[1] - 2*dot*n[1]], dtype=np.float32)\n",
        "\n",
        "    def _move_moving_obstacles(self):\n",
        "        for i in range(self.N):\n",
        "            for j in range(min(self.active_k, self.max_obs)):\n",
        "                if self.obs_type[i, j] == self.TYPE_MCIRC:\n",
        "                    _, x, y, r, _, vx, vy = self.obs_params[i, j]\n",
        "                    x_new = x + vx * self.cfg.dt\n",
        "                    y_new = y + vy * self.cfg.dt\n",
        "                    if x_new - r < self.low or x_new + r > self.high:\n",
        "                        vx = -vx\n",
        "                        x_new = np.clip(x_new, self.low + r + self.cfg.collision_eps, self.high - r - self.cfg.collision_eps)\n",
        "                    if y_new - r < self.low or y_new + r > self.high:\n",
        "                        vy = -vy\n",
        "                        y_new = np.clip(y_new, self.low + r + self.cfg.collision_eps, self.high - r - self.cfg.collision_eps)\n",
        "                    self.obs_params[i, j, 1] = x_new\n",
        "                    self.obs_params[i, j, 2] = y_new\n",
        "                    self.obs_params[i, j, 5] = vx\n",
        "                    self.obs_params[i, j, 6] = vy\n",
        "\n",
        "    def _collide_circle(self, p, v, cx, cy, r):\n",
        "        dx, dy = p[0]-cx, p[1]-cy\n",
        "        dist = math.sqrt(dx*dx + dy*dy) + 1e-8\n",
        "        if dist < r:\n",
        "            n = np.array([dx/dist, dy/dist], dtype=np.float32)\n",
        "            p = np.array([cx + n[0]*(r + self.cfg.collision_eps),\n",
        "                          cy + n[1]*(r + self.cfg.collision_eps)], dtype=np.float32)\n",
        "            v = self._reflect_velocity(v, n)\n",
        "        return p, v\n",
        "\n",
        "    def _collide_rect(self, p, v, cx, cy, w, h):\n",
        "        left, right = cx - w/2, cx + w/2\n",
        "        bottom, top = cy - h/2, cy + h/2\n",
        "        inside = (left < p[0] < right) and (bottom < p[1] < top)\n",
        "        if inside:\n",
        "            pen_left   = abs(p[0] - left)\n",
        "            pen_right  = abs(right - p[0])\n",
        "            pen_bottom = abs(p[1] - bottom)\n",
        "            pen_top    = abs(top - p[1])\n",
        "            mins = [pen_left, pen_right, pen_bottom, pen_top]\n",
        "            idx = int(np.argmin(mins))\n",
        "            if idx == 0:\n",
        "                n = np.array([-1.0, 0.0], dtype=np.float32)\n",
        "                p = np.array([left - self.cfg.collision_eps, p[1]], dtype=np.float32)\n",
        "            elif idx == 1:\n",
        "                n = np.array([ 1.0, 0.0], dtype=np.float32)\n",
        "                p = np.array([right + self.cfg.collision_eps, p[1]], dtype=np.float32)\n",
        "            elif idx == 2:\n",
        "                n = np.array([0.0, -1.0], dtype=np.float32)\n",
        "                p = np.array([p[0], bottom - self.cfg.collision_eps], dtype=np.float32)\n",
        "            else:\n",
        "                n = np.array([0.0,  1.0], dtype=np.float32)\n",
        "                p = np.array([p[0], top + self.cfg.collision_eps], dtype=np.float32)\n",
        "            v = self._reflect_velocity(v, n)\n",
        "        return p, v\n",
        "\n",
        "    def _step_single(self, i, a):\n",
        "        if self.done[i]: return\n",
        "        a = np.clip(a, -1.0, 1.0) * self.cfg.accel_max\n",
        "        self.vel[i] = (1.0 - self.cfg.damping)*self.vel[i] + a * self.cfg.dt\n",
        "        sp = np.linalg.norm(self.vel[i]) + 1e-8\n",
        "        if sp > self.cfg.vmax:\n",
        "            self.vel[i] *= (self.cfg.vmax / sp)\n",
        "        new_pos = self.pos[i] + self.vel[i] * self.cfg.dt\n",
        "\n",
        "        for k in (0,1):\n",
        "            if new_pos[k] < self.low:\n",
        "                new_pos[k] = self.low + self.cfg.collision_eps\n",
        "                self.vel[i][k] = abs(self.vel[i][k])\n",
        "            elif new_pos[k] > self.high:\n",
        "                new_pos[k] = self.high - self.cfg.collision_eps\n",
        "                self.vel[i][k] = -abs(self.vel[i][k])\n",
        "\n",
        "        for j in range(min(self.active_k, self.max_obs)):\n",
        "            typ = self.obs_type[i, j]\n",
        "            _, x, y, s1, s2, vx, vy = self.obs_params[i, j]\n",
        "            if typ == self.TYPE_CIRCLE or typ == self.TYPE_MCIRC:\n",
        "                new_pos, self.vel[i] = self._collide_circle(new_pos, self.vel[i], x, y, s1)\n",
        "            else:\n",
        "                new_pos, self.vel[i] = self._collide_rect(new_pos, self.vel[i], x, y, s1, s2)\n",
        "\n",
        "        self.pos[i] = new_pos\n",
        "        self.t[i]  += 1\n",
        "        d_goal = np.linalg.norm(self.pos[i] - self.goals[i])\n",
        "        if d_goal <= self.cfg.goal_r or self.t[i] >= self.cfg.max_steps:\n",
        "            self.done[i] = True\n",
        "\n",
        "    def step(self, actions_player):\n",
        "        self._move_moving_obstacles()\n",
        "        if isinstance(actions_player, torch.Tensor):\n",
        "            actions_player = actions_player.detach().cpu().numpy()\n",
        "        for i in range(self.N):\n",
        "            self._step_single(i, actions_player[i])\n",
        "\n",
        "    def get_player_obs(self):\n",
        "        p = self.obs_params.copy()\n",
        "        p = p[:, :, [1,2,3,4,5,6,0]]\n",
        "        flat = p.reshape(self.N, -1)\n",
        "        vec_to_goal = self.goals - self.pos\n",
        "        base = np.concatenate([self.pos, self.vel, self.goals, vec_to_goal], axis=1)\n",
        "        obs = np.concatenate([base, flat], axis=1)\n",
        "        return torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
        "\n",
        "    def get_creator_obs(self):\n",
        "        obs = np.concatenate([self.starts, self.goals], axis=1)\n",
        "        return torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
        "\n",
        "    def compute_rewards(self, prev_dists):\n",
        "        new_dists = np.linalg.norm(self.pos - self.goals, axis=1)\n",
        "        progress = prev_dists - new_dists\n",
        "        r = 1.5*progress - 0.005\n",
        "        success = (new_dists <= self.cfg.goal_r)\n",
        "        r = r + (success.astype(np.float32)*1.0)\n",
        "        return r.astype(np.float32), new_dists\n",
        "\n",
        "    def episode_creator_rewards(self):\n",
        "        final_d = np.linalg.norm(self.pos - self.goals, axis=1)\n",
        "        success = (final_d <= self.cfg.goal_r)\n",
        "        base = (1.0 - success.astype(np.float32))\n",
        "        dist_term = (final_d / (self.high - self.low)) * 0.5\n",
        "        penalty = np.zeros(self.N, dtype=np.float32)\n",
        "        for i in range(self.N):\n",
        "            for j in range(min(self.active_k, self.max_obs)):\n",
        "                typ = self.obs_type[i, j]\n",
        "                tc, x, y, s1, s2, vx, vy = self.obs_params[i, j]\n",
        "                if typ == self.TYPE_CIRCLE or typ == self.TYPE_MCIRC:\n",
        "                    if not (self.low <= x-s1 and x+s1 <= self.high and self.low <= y-s1 and y+s1 <= self.high):\n",
        "                        penalty[i] += 0.1\n",
        "                else:\n",
        "                    if not (self.low <= x-s1/2 and x+s1/2 <= self.high and self.low <= y-s2/2 and y+s2/2 <= self.high):\n",
        "                        penalty[i] += 0.1\n",
        "                if typ == self.TYPE_RECT:\n",
        "                    if np.linalg.norm(np.array([x,y]) - self.starts[i]) < (self.cfg.start_r + 0.5*min(s1,s2) + self.cfg.min_clearance):\n",
        "                        penalty[i] += 0.1\n",
        "                    if np.linalg.norm(np.array([x,y]) - self.goals[i]) < (self.cfg.goal_r + 0.5*min(s1,s2) + self.cfg.min_clearance):\n",
        "                        penalty[i] += 0.1\n",
        "                else:\n",
        "                    if np.linalg.norm(np.array([x,y]) - self.starts[i]) < (self.cfg.start_r + s1 + self.cfg.min_clearance):\n",
        "                        penalty[i] += 0.1\n",
        "                    if np.linalg.norm(np.array([x,y]) - self.goals[i]) < (self.cfg.goal_r + s1 + self.cfg.min_clearance):\n",
        "                        penalty[i] += 0.1\n",
        "                if typ == self.TYPE_MCIRC and (abs(vx) + abs(vy)) > 1.0*self.cfg.obs_vmax:\n",
        "                    penalty[i] += 0.1\n",
        "        rew = base + dist_term - penalty\n",
        "        return rew.astype(np.float32)\n",
        "\n",
        "    def render_frame(self, i, figsize=4, path=None):\n",
        "        fig, ax = plt.subplots(figsize=(figsize, figsize))\n",
        "        ax.set_xlim(self.low, self.high); ax.set_ylim(self.low, self.high)\n",
        "        ax.set_aspect('equal'); ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.plot([self.low, self.low, self.high, self.high, self.low],\n",
        "                [self.low, self.high, self.high, self.low, self.low])\n",
        "        for j in range(min(self.active_k, self.max_obs)):\n",
        "            typ = self.obs_type[i, j]\n",
        "            _, x, y, s1, s2, vx, vy = self.obs_params[i, j]\n",
        "            if typ == self.TYPE_CIRCLE:\n",
        "                circ = plt.Circle((x,y), s1, fill=False, linewidth=2)\n",
        "                ax.add_patch(circ)\n",
        "            elif typ == self.TYPE_RECT:\n",
        "                rect = plt.Rectangle((x - s1/2, y - s2/2), s1, s2, fill=False, linewidth=2)\n",
        "                ax.add_patch(rect)\n",
        "            else:\n",
        "                circ = plt.Circle((x,y), s1, fill=False, linewidth=2)\n",
        "                ax.add_patch(circ)\n",
        "                ax.arrow(x, y, vx*0.1, vy*0.1, head_width=0.03, length_includes_head=True)\n",
        "        ax.add_patch(plt.Circle(tuple(self.starts[i]), self.cfg.start_r, color='green', alpha=0.4))\n",
        "        ax.add_patch(plt.Circle(tuple(self.goals[i]),  self.cfg.goal_r,  color='red',   alpha=0.4))\n",
        "        ax.add_patch(plt.Circle(tuple(self.pos[i]), 0.02, color='black'))\n",
        "        if path is not None:\n",
        "            fig.savefig(path, bbox_inches='tight', pad_inches=0.05)\n",
        "            plt.close(fig)\n",
        "        else:\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b394b99",
      "metadata": {
        "id": "8b394b99"
      },
      "source": [
        "## PPO & Utilities (Running Normalization, Adaptive KL, LR Anneal)\n",
        "\n",
        "Both agents use PPO with shared design choices: orthogonal initialization, layer-normalized MLP backbones, entropy regularization, gradient clipping, AdamW, generalized advantage estimation (GAE-Î»), observation and return normalization, linear learning-rate annealing, and adaptive KL early-stop per epoch. We alternate updates: Creator samples a level once per episode batch; the Player collects rollouts on that batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "192668a8",
      "metadata": {
        "id": "192668a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def orthogonal_init(layer, gain=math.sqrt(2)):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.orthogonal_(layer.weight, gain=gain)\n",
        "        if layer.bias is not None:\n",
        "            nn.init.zeros_(layer.bias)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hid=128, out_dim=64, depth=2, act=nn.Tanh, layer_norm=True):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        d = in_dim\n",
        "        for k in range(depth):\n",
        "            lin = nn.Linear(d, hid)\n",
        "            orthogonal_init(lin)\n",
        "            block = [lin]\n",
        "            if layer_norm:\n",
        "                block.append(nn.LayerNorm(hid))\n",
        "            block.append(act())\n",
        "            layers += block\n",
        "            d = hid\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "        self.head = nn.Linear(d, out_dim)\n",
        "        orthogonal_init(self.head, gain=1.0)\n",
        "    def forward(self, x):\n",
        "        return self.head(self.backbone(x))\n",
        "\n",
        "class ActorCriticGaussian(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.body = MLP(obs_dim, hid=hidden, out_dim=hidden, depth=2, layer_norm=True)\n",
        "        self.mu = nn.Linear(hidden, act_dim)\n",
        "        self.v  = nn.Linear(hidden, 1)\n",
        "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
        "        orthogonal_init(self.mu, gain=0.01)\n",
        "        orthogonal_init(self.v,  gain=1.0)\n",
        "    def forward(self, obs):\n",
        "        h = self.body(obs)\n",
        "        mu = self.mu(h)\n",
        "        v  = self.v(h).squeeze(-1)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mu, std, v\n",
        "    def act(self, obs):\n",
        "        mu, std, v = self(obs)\n",
        "        dist = Normal(mu, std)\n",
        "        a = dist.rsample()\n",
        "        logp = dist.log_prob(a).sum(-1)\n",
        "        return a, logp, v\n",
        "    def evaluate(self, obs, actions):\n",
        "        mu, std, v = self(obs)\n",
        "        dist = Normal(mu, std)\n",
        "        logp = dist.log_prob(actions).sum(-1)\n",
        "        entropy = dist.entropy().sum(-1)\n",
        "        return logp, entropy, v\n",
        "\n",
        "@dataclass\n",
        "class PPOConfig:\n",
        "    gamma: float = 0.99\n",
        "    lam: float = 0.95\n",
        "    clip_ratio: float = 0.2\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    ent_coef: float = 0.01\n",
        "    vf_coef: float = 0.5\n",
        "    max_grad_norm: float = 0.5\n",
        "    update_epochs: int = 4\n",
        "    minibatch_size: int = 1024\n",
        "    target_kl: float = 0.02\n",
        "\n",
        "class RunningMeanStd:\n",
        "    def __init__(self, shape):\n",
        "        if shape == ():\n",
        "            self.mean = np.array(0.0, dtype=np.float32)\n",
        "            self.var  = np.array(1.0, dtype=np.float32)\n",
        "        else:\n",
        "            self.mean = np.zeros(shape, dtype=np.float32)\n",
        "            self.var  = np.ones(shape, dtype=np.float32)\n",
        "        self.count = 1e-4\n",
        "    def update(self, x):\n",
        "        x = np.asarray(x, dtype=np.float32)\n",
        "        if x.ndim == 0:\n",
        "            x = x.reshape(1,1)\n",
        "        elif x.ndim == 1:\n",
        "            x = x.reshape(-1, x.shape[-1])\n",
        "        batch_mean = x.mean(axis=0)\n",
        "        batch_var  = x.var(axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * self.count\n",
        "        m_b = batch_var * batch_count\n",
        "        M2 = m_a + m_b + delta**2 * self.count * batch_count / tot_count\n",
        "        new_var = M2 / tot_count\n",
        "        self.mean, self.var, self.count = new_mean, new_var, tot_count\n",
        "    def normalize(self, x):\n",
        "        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)\n",
        "\n",
        "def compute_gae(rewards, values, dones, gamma, lam):\n",
        "    T, N = rewards.shape\n",
        "    adv = np.zeros((T, N), dtype=np.float32)\n",
        "    lastgaelam = np.zeros((N,), dtype=np.float32)\n",
        "    next_value = values[-1]\n",
        "    next_nonterminal = 1.0 - dones[-1]\n",
        "    for t in reversed(range(T)):\n",
        "        delta = rewards[t] + gamma * next_value * next_nonterminal - values[t]\n",
        "        lastgaelam = delta + gamma * lam * next_nonterminal * lastgaelam\n",
        "        adv[t] = lastgaelam\n",
        "        next_value = values[t]\n",
        "        next_nonterminal = 1.0 - dones[t]\n",
        "    ret = adv + values[:-1]\n",
        "    return adv, ret\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7995cca3",
      "metadata": {
        "id": "7995cca3"
      },
      "source": [
        "## Training: Smarter PPO + Simple Curriculum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "11a5c3a3",
      "metadata": {
        "id": "11a5c3a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 num_envs=32,\n",
        "                 steps_per_env=128,\n",
        "                 total_updates=400,\n",
        "                 player_hidden=128,\n",
        "                 creator_hidden=128,\n",
        "                 save_dir=None,\n",
        "                 init_active_obstacles=3):\n",
        "        self.level_cfg = LevelConfig()\n",
        "        self.env = PlayerCreatorEnvVec(num_envs=num_envs, level_cfg=self.level_cfg, active_obstacles=init_active_obstacles)\n",
        "        self.num_envs = num_envs\n",
        "        self.steps_per_env = steps_per_env\n",
        "        self.total_updates = total_updates\n",
        "\n",
        "        self.obs_p_dim = self.env.dim_obs_player\n",
        "        self.obs_c_dim = self.env.dim_obs_creator\n",
        "        self.player = ActorCriticGaussian(self.obs_p_dim, act_dim=2, hidden=player_hidden).to(device)\n",
        "        self.creator = ActorCriticGaussian(self.obs_c_dim, act_dim=self.level_cfg.max_obstacles*7, hidden=creator_hidden).to(device)\n",
        "\n",
        "        self.cfg_p = PPOConfig(ent_coef=0.01, target_kl=0.02)\n",
        "        self.cfg_c = PPOConfig(lr=3e-4, ent_coef=0.02, target_kl=0.02)\n",
        "\n",
        "        self.opt_p = torch.optim.AdamW(self.player.parameters(), lr=self.cfg_p.lr, weight_decay=self.cfg_p.weight_decay)\n",
        "        self.opt_c = torch.optim.AdamW(self.creator.parameters(), lr=self.cfg_c.lr, weight_decay=self.cfg_c.weight_decay)\n",
        "\n",
        "        self.save_dir = save_dir or (\"/content\" if os.path.exists(\"/content\") else \".\")\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "        self.best_eval = -1e9\n",
        "\n",
        "        self.obs_rms = RunningMeanStd(self.obs_p_dim)\n",
        "        self.ret_rms = RunningMeanStd(())\n",
        "\n",
        "        self.base_lr_p = self.cfg_p.lr\n",
        "        self.base_lr_c = self.cfg_c.lr\n",
        "\n",
        "    def _anneal_lrs(self, upd):\n",
        "        frac = 1.0 - (upd - 1) / max(1, self.total_updates)\n",
        "        for g in self.opt_p.param_groups:\n",
        "            g['lr'] = self.base_lr_p * frac\n",
        "        for g in self.opt_c.param_groups:\n",
        "            g['lr'] = self.base_lr_c * frac\n",
        "\n",
        "    def rollout(self):\n",
        "        obs_c = self.env.get_creator_obs()\n",
        "        with torch.no_grad():\n",
        "            a_c, logp_c, v_c = self.creator.act(obs_c)\n",
        "        self.env.set_obstacles_from_creator_action(torch.tanh(a_c))\n",
        "\n",
        "        T = self.steps_per_env\n",
        "        obs_buf = torch.zeros((T, self.num_envs, self.obs_p_dim), dtype=torch.float32, device=device)\n",
        "        act_buf = torch.zeros((T, self.num_envs, 2), dtype=torch.float32, device=device)\n",
        "        logp_buf= torch.zeros((T, self.num_envs), dtype=torch.float32, device=device)\n",
        "        val_buf = torch.zeros((T+1, self.num_envs), dtype=torch.float32, device=device)\n",
        "        rew_buf = torch.zeros((T, self.num_envs), dtype=torch.float32, device=device)\n",
        "        done_buf= torch.zeros((T+1, self.num_envs), dtype=torch.float32, device=device)\n",
        "\n",
        "        self.env.reset_players()\n",
        "        prev_dists = np.linalg.norm(self.env.pos - self.env.goals, axis=1)\n",
        "\n",
        "        for t in range(T):\n",
        "            obs_p = self.env.get_player_obs()\n",
        "            self.obs_rms.update(obs_p.detach().cpu().numpy().reshape(self.num_envs, -1))\n",
        "            obs_norm = torch.as_tensor(self.obs_rms.normalize(obs_p.detach().cpu().numpy().reshape(self.num_envs, -1)), device=device, dtype=torch.float32)\n",
        "            with torch.no_grad():\n",
        "                a_p, logp_p, v_p = self.player.act(obs_norm)\n",
        "            a_clip = torch.tanh(a_p)\n",
        "            self.env.step(a_clip)\n",
        "            rewards, new_dists = self.env.compute_rewards(prev_dists)\n",
        "            prev_dists = new_dists\n",
        "            obs_buf[t] = obs_p\n",
        "            act_buf[t] = a_p\n",
        "            logp_buf[t] = logp_p\n",
        "            val_buf[t] = v_p\n",
        "            rew_buf[t] = torch.as_tensor(rewards, device=device)\n",
        "            done_buf[t] = torch.as_tensor(self.env.done.astype(np.float32), device=device)\n",
        "            if np.all(self.env.done):\n",
        "                for tp in range(t+1, T):\n",
        "                    obs_buf[tp] = obs_buf[t]\n",
        "                    act_buf[tp] = act_buf[t]\n",
        "                    logp_buf[tp]= logp_buf[t]\n",
        "                    rew_buf[tp] = 0.0\n",
        "                    done_buf[tp]= 1.0\n",
        "                break\n",
        "\n",
        "        obs_p = self.env.get_player_obs()\n",
        "        obs_norm = torch.as_tensor(self.obs_rms.normalize(obs_p.detach().cpu().numpy().reshape(self.num_envs, -1)), device=device, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            _, _, v_last = self.player(obs_norm)\n",
        "        val_buf[T] = v_last\n",
        "        done_buf[T] = torch.as_tensor(self.env.done.astype(np.float32), device=device)\n",
        "\n",
        "        c_rewards = self.env.episode_creator_rewards()\n",
        "        c_batch = dict(obs=obs_c.detach(),\n",
        "                       act=a_c.detach(),\n",
        "                       logp=logp_c.detach(),\n",
        "                       val=v_c.detach(),\n",
        "                       rew=torch.as_tensor(c_rewards, device=device))\n",
        "        return {\"player\": dict(obs=obs_buf, act=act_buf, logp=logp_buf, val=val_buf, rew=rew_buf, done=done_buf),\n",
        "                \"creator\": c_batch}\n",
        "\n",
        "    def _ppo_update(self, ac: ActorCriticGaussian, opt, batch, cfg: PPOConfig, normalize_obs=False):\n",
        "        obs = batch[\"obs\"].reshape(-1, batch[\"obs\"].shape[-1])\n",
        "        if normalize_obs:\n",
        "            self.obs_rms.update(obs.detach().cpu().numpy())\n",
        "            obs = torch.as_tensor(self.obs_rms.normalize(obs.detach().cpu().numpy()), device=device, dtype=torch.float32)\n",
        "        act = batch[\"act\"].reshape(-1, batch[\"act\"].shape[-1])\n",
        "        old_logp = batch[\"logp\"].reshape(-1)\n",
        "        adv = batch[\"adv\"].reshape(-1)\n",
        "        ret = batch[\"ret\"].reshape(-1)\n",
        "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "        inds = np.arange(obs.shape[0])\n",
        "        for _ in range(cfg.update_epochs):\n",
        "            np.random.shuffle(inds)\n",
        "            approx_kl_accum, num_mb = 0.0, 0\n",
        "            for start in range(0, len(inds), cfg.minibatch_size):\n",
        "                mb = inds[start:start+cfg.minibatch_size]\n",
        "                logp_new, entropy, v_pred = ac.evaluate(obs[mb], act[mb])\n",
        "                ratio = torch.exp(logp_new - old_logp[mb])\n",
        "                surr1 = ratio * adv[mb]\n",
        "                surr2 = torch.clamp(ratio, 1.0 - cfg.clip_ratio, 1.0 + cfg.clip_ratio) * adv[mb]\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                v_loss = F.mse_loss(v_pred, ret[mb])\n",
        "                loss = policy_loss + cfg.vf_coef*v_loss - cfg.ent_coef*entropy.mean()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(ac.parameters(), cfg.max_grad_norm)\n",
        "                opt.step()\n",
        "                approx_kl = (old_logp[mb] - logp_new).mean().item()\n",
        "                approx_kl_accum += approx_kl; num_mb += 1\n",
        "            if num_mb > 0 and (approx_kl_accum/num_mb) > cfg.target_kl:\n",
        "                break\n",
        "\n",
        "    def _ppo_update_creator(self, ac: ActorCriticGaussian, opt, batch, cfg: PPOConfig):\n",
        "        obs = batch[\"obs\"]\n",
        "        act = batch[\"act\"]\n",
        "        old_logp = batch[\"logp\"]\n",
        "        rew = batch[\"rew\"]\n",
        "        val = batch[\"val\"]\n",
        "        adv = (rew - val).detach()\n",
        "        ret = rew.detach()\n",
        "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "        inds = np.arange(obs.shape[0])\n",
        "        for _ in range(cfg.update_epochs):\n",
        "            np.random.shuffle(inds)\n",
        "            approx_kl_accum, num_mb = 0.0, 0\n",
        "            for start in range(0, len(inds), cfg.minibatch_size):\n",
        "                mb = inds[start:start+cfg.minibatch_size]\n",
        "                logp_new, entropy, v_pred = ac.evaluate(obs[mb], act[mb])\n",
        "                ratio = torch.exp(logp_new - old_logp[mb])\n",
        "                surr1 = ratio * adv[mb]\n",
        "                surr2 = torch.clamp(ratio, 1.0 - cfg.clip_ratio, 1.0 + cfg.clip_ratio) * adv[mb]\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                v_loss = F.mse_loss(v_pred, ret[mb])\n",
        "                loss = policy_loss + cfg.vf_coef*v_loss - cfg.ent_coef*entropy.mean()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(ac.parameters(), cfg.max_grad_norm)\n",
        "                opt.step()\n",
        "                approx_kl = (old_logp[mb] - logp_new).mean().item()\n",
        "                approx_kl_accum += approx_kl; num_mb += 1\n",
        "            if num_mb > 0 and (approx_kl_accum/num_mb) > cfg.target_kl:\n",
        "                break\n",
        "\n",
        "    def evaluate(self, episodes=16):\n",
        "        env = PlayerCreatorEnvVec(num_envs=episodes, level_cfg=self.level_cfg, active_obstacles=self.env.active_k)\n",
        "        with torch.no_grad():\n",
        "            c_obs = env.get_creator_obs()\n",
        "            a_c, _, _ = self.creator.act(c_obs)\n",
        "            env.set_obstacles_from_creator_action(torch.tanh(a_c))\n",
        "        env.reset_players()\n",
        "        prev_d = np.linalg.norm(env.pos - env.goals, axis=1)\n",
        "        for t in range(self.level_cfg.max_steps):\n",
        "            obs = env.get_player_obs()\n",
        "            obs_n = torch.as_tensor(self.obs_rms.normalize(obs.detach().cpu().numpy().reshape(episodes, -1)), device=device, dtype=torch.float32)\n",
        "            with torch.no_grad():\n",
        "                a, _, _ = self.player.act(obs_n)\n",
        "            env.step(torch.tanh(a))\n",
        "            r, prev_d = env.compute_rewards(prev_d)\n",
        "        final_d = np.linalg.norm(env.pos - env.goals, axis=1)\n",
        "        successes = (final_d <= self.level_cfg.goal_r).astype(np.int32)\n",
        "        return successes.mean(), final_d.mean()\n",
        "\n",
        "    def train(self, log_every=10, eval_every=20, save_name=\"player_creator_ppo_v2\"):\n",
        "        for upd in range(1, self.total_updates+1):\n",
        "            self._anneal_lrs(upd)\n",
        "            rollout = self.rollout()\n",
        "            P = rollout[\"player\"]\n",
        "            with torch.no_grad():\n",
        "                adv, ret = compute_gae(\n",
        "                    rewards = P[\"rew\"].cpu().numpy(),\n",
        "                    values  = P[\"val\"].cpu().numpy(),\n",
        "                    dones   = P[\"done\"].cpu().numpy(),\n",
        "                    gamma=0.99, lam=0.95\n",
        "                )\n",
        "            self.ret_rms.update(ret.reshape(-1,1))\n",
        "            ret_norm = (ret - ret.mean()) / (np.sqrt(self.ret_rms.var) + 1e-8)\n",
        "            player_batch = dict(obs=P[\"obs\"], act=P[\"act\"], logp=P[\"logp\"],\n",
        "                                adv=torch.as_tensor(adv, device=device),\n",
        "                                ret=torch.as_tensor(ret_norm, device=device))\n",
        "            self._ppo_update(self.player, self.opt_p, player_batch, PPOConfig(), normalize_obs=True)\n",
        "\n",
        "            C = rollout[\"creator\"]\n",
        "            creator_batch = dict(obs=C[\"obs\"], act=C[\"act\"], logp=C[\"logp\"], rew=C[\"rew\"], val=C[\"val\"])\n",
        "            self._ppo_update_creator(self.creator, self.opt_c, creator_batch, PPOConfig())\n",
        "\n",
        "            if upd % log_every == 0:\n",
        "                with torch.no_grad():\n",
        "                    p_ent = Normal(torch.zeros_like(self.player.log_std), torch.exp(self.player.log_std)).entropy().sum().item()\n",
        "                    c_ent = Normal(torch.zeros_like(self.creator.log_std), torch.exp(self.creator.log_std)).entropy().sum().item()\n",
        "                print(f\"[Update {upd}] PlayerEnt {p_ent:.2f} | CreatorEnt {c_ent:.2f} | active_obs={self.env.active_k}\")\n",
        "\n",
        "            if upd % eval_every == 0:\n",
        "                succ, fdist = self.evaluate(episodes=16)\n",
        "                score = succ*1.0 - fdist\n",
        "                print(f\"  Eval -> success_rate={succ:.2f}, final_dist={fdist:.3f}\")\n",
        "                if succ > 0.70 and self.env.active_k < self.level_cfg.max_obstacles:\n",
        "                    self.env.active_k += 1; self.env.n_obs[:] = self.env.active_k\n",
        "                    print(f\"  ðŸ“ˆ Curriculum: increased active obstacles to {self.env.active_k}\")\n",
        "                elif succ < 0.30 and self.env.active_k > 2:\n",
        "                    self.env.active_k -= 1; self.env.n_obs[:] = self.env.active_k\n",
        "                    print(f\"  ðŸ“‰ Curriculum: decreased active obstacles to {self.env.active_k}\")\n",
        "                if score > self.best_eval:\n",
        "                    self.best_eval = score\n",
        "                    torch.save(self.player.state_dict(), os.path.join(self.save_dir, f\"{save_name}_player.pt\"))\n",
        "                    torch.save(self.creator.state_dict(), os.path.join(self.save_dir, f\"{save_name}_creator.pt\"))\n",
        "                    with open(os.path.join(self.save_dir, f\"{save_name}_meta.json\"), \"w\") as f:\n",
        "                        json.dump({\"best_score\": float(self.best_eval), \"update\": upd, \"active_k\": self.env.active_k}, f)\n",
        "                    print(\"  âœ… Saved new best checkpoints.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe2239c",
      "metadata": {
        "id": "fbe2239c"
      },
      "source": [
        "## (Optional) Quick Tiny Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eb509816",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb509816",
        "outputId": "1c25a7fb-2ba0-4fef-8aed-8c2a4a694789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Update 5] PlayerEnt 2.84 | CreatorEnt 59.58 | active_obs=3\n",
            "[Update 10] PlayerEnt 2.84 | CreatorEnt 59.57 | active_obs=3\n",
            "  Eval -> success_rate=0.06, final_dist=0.796\n",
            "  ðŸ“‰ Curriculum: decreased active obstacles to 2\n",
            "  âœ… Saved new best checkpoints.\n",
            "[Update 15] PlayerEnt 2.84 | CreatorEnt 59.58 | active_obs=2\n",
            "[Update 20] PlayerEnt 2.84 | CreatorEnt 59.58 | active_obs=2\n",
            "  Eval -> success_rate=0.19, final_dist=0.541\n",
            "  âœ… Saved new best checkpoints.\n",
            "Tiny run complete. Best score so far: -0.35351258516311646\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer = Trainer(num_envs=16, steps_per_env=96, total_updates=20, init_active_obstacles=3)\n",
        "    trainer.train(log_every=5, eval_every=10, save_name=\"player_creator_ppo_v2\")\n",
        "    print(\"Tiny run complete. Best score so far:\", trainer.best_eval)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d261e6",
      "metadata": {
        "id": "41d261e6"
      },
      "source": [
        "## Sim Helper + GIF Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5a8de70b",
      "metadata": {
        "id": "5a8de70b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def simulate_and_gif(player, creator, level_cfg=None, seed=0, steps=220, figsize=4, tmpdir=\".\", fname=\"episode_v2.gif\", active_k=4):\n",
        "    level_cfg = level_cfg or LevelConfig()\n",
        "    env = PlayerCreatorEnvVec(num_envs=1, level_cfg=level_cfg, active_obstacles=active_k)\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    with torch.no_grad():\n",
        "        c_obs = env.get_creator_obs()\n",
        "        a_c, _, _ = creator.act(c_obs)\n",
        "        env.set_obstacles_from_creator_action(torch.tanh(a_c))\n",
        "    env.reset_players()\n",
        "    prev_d = np.linalg.norm(env.pos - env.goals, axis=1)\n",
        "    frames = []\n",
        "    for t in range(min(steps, level_cfg.max_steps)):\n",
        "        path = os.path.join(tmpdir, f\"frame_{t:04d}.png\")\n",
        "        env.render_frame(0, figsize=figsize, path=path)\n",
        "        frames.append(imageio.imread(path))\n",
        "        obs = env.get_player_obs()\n",
        "        obs_np = obs.detach().cpu().numpy()\n",
        "        m, s = obs_np.mean(0), obs_np.std(0) + 1e-8\n",
        "        obs_n = torch.as_tensor((obs_np - m)/s, device=device, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            a, _, _ = player.act(obs_n)\n",
        "        env.step(torch.tanh(a))\n",
        "        r, prev_d = env.compute_rewards(prev_d)\n",
        "        if env.done[0]:\n",
        "            path = os.path.join(tmpdir, f\"frame_{t+1:04d}.png\")\n",
        "            env.render_frame(0, figsize=figsize, path=path)\n",
        "            frames.append(imageio.imread(path))\n",
        "            break\n",
        "    gif_path = os.path.join(tmpdir, fname)\n",
        "    imageio.mimsave(gif_path, frames, duration=0.06)\n",
        "    return gif_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e944148",
      "metadata": {
        "id": "3e944148"
      },
      "source": [
        "## Gradio Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7719659d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "7719659d",
        "outputId": "f7bd6261-3afa-4ad8-8f94-83daef5e863a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://745b84821204536ac0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://745b84821204536ac0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "import os, json\n",
        "import gradio as gr\n",
        "\n",
        "BASE_DIR = \"/content\" if os.path.exists(\"/content\") else \".\"\n",
        "PLAYER_CKPT = os.path.join(BASE_DIR, \"player_creator_ppo_v2_player.pt\")\n",
        "CREATOR_CKPT = os.path.join(BASE_DIR, \"player_creator_ppo_v2_creator.pt\")\n",
        "\n",
        "tmp_env = PlayerCreatorEnvVec(num_envs=1, level_cfg=LevelConfig(), active_obstacles=4)\n",
        "player_demo = ActorCriticGaussian(tmp_env.dim_obs_player, 2, hidden=128).to(device)\n",
        "creator_demo = ActorCriticGaussian(tmp_env.dim_obs_creator, tmp_env.max_obs*7, hidden=128).to(device)\n",
        "\n",
        "def load_if_exists():\n",
        "    if os.path.exists(PLAYER_CKPT) and os.path.exists(CREATOR_CKPT):\n",
        "        player_demo.load_state_dict(torch.load(PLAYER_CKPT, map_location=device))\n",
        "        creator_demo.load_state_dict(torch.load(CREATOR_CKPT, map_location=device))\n",
        "        return \"Loaded saved checkpoints.\"\n",
        "    else:\n",
        "        return \"No checkpoints found yet. Run some training above first.\"\n",
        "\n",
        "status_msg = load_if_exists()\n",
        "\n",
        "def run_demo(seed: int, steps: int, figsize: float, active_obs: int):\n",
        "    cfg = LevelConfig(max_obstacles=6, max_steps=int(steps))\n",
        "    env_probe = PlayerCreatorEnvVec(1, cfg, active_obstacles=int(active_obs))\n",
        "    local_player = ActorCriticGaussian(env_probe.dim_obs_player, 2, hidden=128).to(device)\n",
        "    local_creator= ActorCriticGaussian(env_probe.dim_obs_creator, env_probe.max_obs*7, hidden=128).to(device)\n",
        "    try:\n",
        "        if os.path.exists(PLAYER_CKPT):\n",
        "            local_player.load_state_dict(torch.load(PLAYER_CKPT, map_location=device), strict=False)\n",
        "        if os.path.exists(CREATOR_CKPT):\n",
        "            local_creator.load_state_dict(torch.load(CREATOR_CKPT, map_location=device), strict=False)\n",
        "    except Exception as e:\n",
        "        print(\"Warn: loading checkpoints:\", e)\n",
        "    out_dir = \"/content\" if os.path.exists(\"/content\") else \".\"\n",
        "    gif_path = simulate_and_gif(local_player, local_creator, level_cfg=cfg, seed=seed, steps=steps, figsize=figsize, tmpdir=out_dir, fname=\"episode_v2.gif\", active_k=int(active_obs))\n",
        "    return gif_path, f\"Seed {seed} â€¢ Steps {steps} â€¢ Active Obstacles {active_obs}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Player vs Creator â€” v2 Demo (circles, rectangles, moving circles)\")\n",
        "    gr.Markdown(status_msg)\n",
        "    with gr.Row():\n",
        "        seed = gr.Slider(0, 9999, step=1, value=0, label=\"Random Seed\")\n",
        "        steps = gr.Slider(100, 400, step=10, value=220, label=\"Max Steps\")\n",
        "    with gr.Row():\n",
        "        active_obs = gr.Slider(1, 6, step=1, value=4, label=\"Active Obstacles (Curriculum)\")\n",
        "        figsize = gr.Slider(3.0, 6.0, step=0.5, value=4.0, label=\"Figure Size (inches)\")\n",
        "    run_btn = gr.Button(\"Generate Episode\")\n",
        "    out_img = gr.Image(type=\"filepath\", label=\"Episode GIF\")\n",
        "    out_txt = gr.Textbox(label=\"Episode Info\", interactive=False)\n",
        "    run_btn.click(run_demo, inputs=[seed, steps, figsize, active_obs], outputs=[out_img, out_txt])\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}